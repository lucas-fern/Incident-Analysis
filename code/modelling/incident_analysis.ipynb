{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incident Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Neural network model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory and Client Setup\n",
    "Configure with data directory and current client.\n",
    "\n",
    "The program generates models for each client individually for privacy reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r'../../data'\n",
    "\n",
    "# Change client in these statements\n",
    "COMPANY = 'geotec'\n",
    "from clients import Geotec as client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the set of text based and categorical predictors for later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRS = set(j for i in client for j in i.keys())\n",
    "TEXT_PREDICTORS = set(['description'])\n",
    "CATEGORICAL_PREDICTORS = set(i for i in client.incident_mapping \n",
    "                             if i not in TEXT_PREDICTORS) - set(['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Uses the mapping between excel columns and predictors (provided in `clients.py`) to extract the required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comma separated list of the required excel columns from each sheet.\n",
    "incident_excel_cols = ','.join(client.incident_mapping.values())\n",
    "action_excel_cols = ','.join(client.action_mapping.values())\n",
    "factor_excel_cols = ','.join(client.factor_mapping.values())\n",
    "\n",
    "# Get the corresponding column names in the order that they appear in the excel sheet.\n",
    "def sort_cols(mapping_dict):\n",
    "    return [k for k, v in sorted(mapping_dict.items(), key=lambda x: x[1])]\n",
    "\n",
    "incident_names = sort_cols(client.incident_mapping)\n",
    "action_names = sort_cols(client.action_mapping)\n",
    "factor_names = sort_cols(client.factor_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/lucas/Documents/Lucidity/incident_analysis/code/.venv/lib/python3.9/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/mnt/c/Users/lucas/Documents/Lucidity/incident_analysis/code/.venv/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "incidents = pd.read_excel(f'{DATA_DIR}/{COMPANY}-incidents.xlsx', \n",
    "                          usecols=incident_excel_cols, skiprows=9, names=incident_names)\n",
    "actions = pd.read_excel(f'{DATA_DIR}/{COMPANY}-actions.xlsx', \n",
    "                        usecols=action_excel_cols, skiprows=6, names=action_names)\n",
    "factors = pd.read_excel(f'{DATA_DIR}/{COMPANY}-factors.xlsx', \n",
    "                        usecols=factor_excel_cols, skiprows=5, names=factor_names)\n",
    "\n",
    "def process_ids(df):\n",
    "    \"\"\"Drops cols with no id's, sets integer id's, and mades id the index.\"\"\"\n",
    "    df = df.dropna(subset=['id'])\n",
    "    df.loc[:,'id'] = df.loc[:,'id'].astype('int64')\n",
    "    return df.set_index('id')\n",
    "\n",
    "incidents = process_ids(incidents)\n",
    "actions = process_ids(actions)\n",
    "full_factors = process_ids(factors)\n",
    "factors = full_factors.drop(['factor-text'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "Performs one-hot encoding on the categorical attributes to prepare the instances for input into ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_categoricals(df, categoricals: list[str], \n",
    "                         dummy_na=True, na_sentinel=None) -> tuple[pd.DataFrame, dict]:\n",
    "    # Remove the index ID (replaced on return)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Stores the mapping between new columns and existing categories\n",
    "    mappings = {}\n",
    "    for column in categoricals:\n",
    "        # Get integer categories\n",
    "        factorised, mapping = pd.factorize(df[column], na_sentinel=na_sentinel)\n",
    "        mappings[column] = mapping\n",
    "\n",
    "        # One-hot encode\n",
    "        dummies = pd.get_dummies(factorised, dummy_na=dummy_na, prefix=column)\n",
    "\n",
    "        # Replace the existing categorical col with the one-hots\n",
    "        df = df.drop([column], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "        dummies = dummies.reset_index(drop=True)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "    return df.set_index('id'), mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy of the incidents dataframe for ML\n",
    "df = incidents.copy()\n",
    "df, mapping = one_hot_categoricals(df, CATEGORICAL_PREDICTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of actions associated with each incident\n",
    "temp = incidents.join(actions, on='id')\n",
    "n_actions = temp.groupby('id').count()['action-id']\n",
    "\n",
    "# Repeat for factors\n",
    "temp = incidents.join(factors, on='id')\n",
    "n_factors = temp.groupby('id').count()['factor-level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the factor levels - possible memory issues with ridiculous number of factors\n",
    "temp, factor_mapping = one_hot_categoricals(temp, ['factor-level'], dummy_na=False, na_sentinel=-1)\n",
    "\n",
    "# factor_codes has one row for each instance, and 0-many columns with 1's representing\n",
    "# boolean flags of whether each factor (column) is present for that instance\n",
    "factor_cols = [col for col in temp.columns if 'factor-level_' in col]\n",
    "factor_codes = temp[factor_cols].groupby('id').sum()\n",
    "\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the target columns\n",
    "df = df.join(factor_codes, on='id')\n",
    "df['n_actions'] = n_actions\n",
    "df['n_factors'] = n_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding\n",
    "Uses a prebuilt text embedding model from Google to convert each text based predictor into a 50-dimensional vector, where nearby vectors in this 50-D space are assumed to be generated from strings with similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "embedding_dim = 50\n",
    "embedding_model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "embedding_layer = hub.KerasLayer(embedding_model, input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(df, texts: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe and a list of n text columns. Returns a dataframe with\n",
    "    those n text columns replaced by 50n columns containing the aforementioned\n",
    "    text embedding.\n",
    "    \"\"\"\n",
    "    for column in texts:\n",
    "        res = np.zeros((len(df[column]), embedding_dim))\n",
    "\n",
    "        # Iterate over all the rows of the text column\n",
    "        for idx, txt in df.reset_index()[column].items():\n",
    "            if type(txt) is not str:\n",
    "                continue\n",
    "            \n",
    "            # If the cell is text, perform and store the embedding\n",
    "            res[idx] = embedding_layer([txt])[0]\n",
    "\n",
    "        # Add the embedding back into the dataframe\n",
    "        df = df.drop([column], axis=1)\n",
    "        col_names = [f'{column}_{i}' for i in range(50)]\n",
    "        df[col_names] = res\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the embedding and convert all the columns to floats now that all are numeric.\n",
    "df = embed_text(df, TEXT_PREDICTORS)\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save to excel for viewing\n",
    "# df.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "### Train / Val / Test Split\n",
    "Split the data for training, validation, and testing of the ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, temp = train_test_split(df, train_size=0.7, random_state=22)\n",
    "val, test = train_test_split(temp, train_size=0.5, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df):\n",
    "    \"\"\"\n",
    "    Takes a dataframe formatted from above, and splits it into the required\n",
    "    numeric and categorical predictors and targets for analysis.\n",
    "\n",
    "    Note that the categorical and numeric return values will not always have the\n",
    "    same amount of rows, since instances without factors are not included in the\n",
    "    categorical dataset (since the categories are factor levels and these\n",
    "    have no factors.)\n",
    "    \"\"\"\n",
    "    # Retain only columns with factors for the categorical dataset\n",
    "    cat_df = df[df['factor-level_-1'] == 0]\n",
    "    cat_df = cat_df.drop('factor-level_-1', axis=1)\n",
    "\n",
    "    # Split the predictors and targets\n",
    "    y_cat = cat_df[[col for col in cat_df.columns if 'factor-level_' in col]]\n",
    "    y_num = df[['n_actions', 'n_factors']]\n",
    "    X_cat = cat_df[[col for col in cat_df.columns if col not in y_num.columns and col not in y_cat.columns]]\n",
    "    X_num = df[[col for col in df.columns if col not in y_num.columns and col not in y_cat.columns]]\n",
    "\n",
    "    return X_cat, X_num, y_cat, y_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the split on each of the train, validation, and test datasets\n",
    "X_cat_train, X_num_train, y_cat_train, y_num_train = split_xy(train)\n",
    "X_cat_val, X_num_val, y_cat_val, y_num_val = split_xy(val)\n",
    "X_cat_test, X_num_test, y_cat_test, y_num_test = split_xy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Models\n",
    "#### Numeric Targets\n",
    "A model to predict the number of factors and actions for a provided instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses an input layer, two fully-connected hidden layers, and a 2D output layer\n",
    "num_model = Sequential([\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(80, activation='relu'),\n",
    "    Dense(2, activation='relu')  # ReLU since the n_{actions, factors} are +'ve\n",
    "])\n",
    "\n",
    "num_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 22:19:26.915205: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 3s 9ms/step - loss: 1.0366 - val_loss: 0.4301\n",
      "Epoch 2/200\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.6999 - val_loss: 0.3065\n",
      "Epoch 3/200\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.6179 - val_loss: 0.3108\n",
      "Epoch 4/200\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5852 - val_loss: 0.2808\n",
      "Epoch 5/200\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5531 - val_loss: 0.2949\n",
      "Epoch 6/200\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5364 - val_loss: 0.2890\n",
      "Epoch 7/200\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5020 - val_loss: 0.3231\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "history = num_model.fit(X_num_train,\n",
    "                        y_num_train,\n",
    "                        epochs=200,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_num_val, y_num_val),\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a very similar model to predict the most likely factor levels\n",
    "cat_model = Sequential([\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(80, activation='relu'),\n",
    "    # Use a softmax activation on the output for categorical probabilities\n",
    "    Dense(len(y_cat_train.columns), activation='softmax')\n",
    "])\n",
    "\n",
    "cat_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0556 - val_loss: 0.0518\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0539 - val_loss: 0.0503\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0523 - val_loss: 0.0498\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0510 - val_loss: 0.0489\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0494 - val_loss: 0.0485\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0484 - val_loss: 0.0483\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0472 - val_loss: 0.0482\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0465 - val_loss: 0.0483\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0453 - val_loss: 0.0484\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0446 - val_loss: 0.0486\n"
     ]
    }
   ],
   "source": [
    "# Train the categorical model stopping at minimum validation loss\n",
    "history = cat_model.fit(X_cat_train,\n",
    "                        y_cat_train,\n",
    "                        epochs=200,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_cat_val, y_cat_val),\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Get Predictions for an Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_text(factor_code):\n",
    "    \"\"\"Gets the last recorded factor description for a given factor code.\"\"\"\n",
    "    return full_factors[full_factors['factor-level'] == factor_code].iloc[-1]['factor-text']\n",
    "\n",
    "def reverse_factor_mapping(indices):\n",
    "    \"\"\"\n",
    "    Takes a set of integer factor codes (factor codes which have been \n",
    "    transformed from the original floating point codes into 0...n integers \n",
    "    through one-hot encoding) and returns the corresponding original factor\n",
    "    codes and factor text.\n",
    "    \"\"\"\n",
    "    reverse_map = factor_mapping['factor-level']\n",
    "    factor_codes = [reverse_map[idx] for idx in indices]\n",
    "\n",
    "    return [(code, factor_text(code)) for code in factor_codes]\n",
    "\n",
    "def get_predictions(cat_xs, num_xs, top=5, get_truth=True, incident_src=None):\n",
    "    \"\"\"\n",
    "    Takes two dataframes of categorical and numeric test instances and gets the \n",
    "    neural network predictions from both models.\n",
    "    \n",
    "    Returns a dictionary with:\n",
    "        Keys:\n",
    "            The keys from the input dataframe.\n",
    "        Values:\n",
    "            A dictionary containing:\n",
    "                - description: the provided text description\n",
    "                - predicted factors: a list of the top 5 (default) factors most\n",
    "                    likely to be included based on the provided input data (as\n",
    "                    predicted by the categorical neural network model.)\n",
    "                - predicted number of actions: as titled\n",
    "                - predicted number of factors: as titled\n",
    "                - [if get_truth] true factors: the true factor numbers and text\n",
    "                - [if get_truth] true number of actions: as titled\n",
    "                - [if get_truth] true number of factors: as titled\n",
    "    \"\"\"\n",
    "    cat_predictions = cat_model.predict(cat_xs)\n",
    "    num_predictions = num_model.predict(num_xs)\n",
    "\n",
    "    if incident_src is None:\n",
    "        incident_src = incidents\n",
    "\n",
    "    res = {}\n",
    "    for idx, pred in enumerate(cat_predictions):\n",
    "        item_id = cat_xs.index[idx]\n",
    "\n",
    "        # Get the indices of the factors with the largest probability\n",
    "        top_indices = np.argsort(pred)[:-top-1:-1]\n",
    "\n",
    "        # Get the probabilities of each index\n",
    "        top_probs = cat_predictions[idx][top_indices]\n",
    "        \n",
    "        # Get the relevant factor text\n",
    "        pred_text = reverse_factor_mapping(top_indices)\n",
    "\n",
    "        # Add the information to the dictionary\n",
    "        res[item_id] = {\n",
    "            'description': incident_src.loc[item_id]['description'], \n",
    "            'predicted factors': list(zip(pred_text, top_probs)), \n",
    "            'predicted number of actions': num_predictions[idx][0],\n",
    "            'predicted number of factors': num_predictions[idx][1]\n",
    "        }\n",
    "\n",
    "        if get_truth:\n",
    "            # Get information about the correct factor labels\n",
    "            true_row = df.loc[item_id][[col for col in df.columns if 'factor-level_' in col]]\n",
    "            true_factors = true_row[true_row == 1].index\n",
    "            true_codes = [int(factor.split('_')[-1]) for factor in true_factors]\n",
    "\n",
    "            true_text = reverse_factor_mapping(true_codes)\n",
    "\n",
    "            res[item_id]['true factors'] = true_text\n",
    "\n",
    "            res[item_id]['true number of actions'] = df.loc[item_id, 'n_actions']\n",
    "            res[item_id]['true number of factors'] = df.loc[item_id, 'n_factors']\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `predicted factors` output contains a confidence level for the prediction. These can (kind of) be considered as probabilities that a given factor will be active on the instance (though not in a rigorous statistical sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{25765: {'description': 'IP was lifting a 20kg drum from the tray of truck, in '\n",
      "                        'doing so IP felt a pop in his right wrist accompanied '\n",
      "                        'by pain. IP stopped work and sought his supervisor, '\n",
      "                        'who took him to the medic for review and treatment.',\n",
      "         'predicted factors': [((5.0,\n",
      "                                 '5. Incorrect body position in relation to '\n",
      "                                 'work'),\n",
      "                                0.26198667),\n",
      "                               ((1.1, 'Job planning or instruction inadequate'),\n",
      "                                0.20840245),\n",
      "                               ((1.2, 'Inattention to details of job'),\n",
      "                                0.113459766),\n",
      "                               ((2.1, 'Rules, procedures or SWMS not followed'),\n",
      "                                0.11144313),\n",
      "                               ((13.0, 'Other Contributing Factors'),\n",
      "                                0.07050478)],\n",
      "         'predicted number of actions': 1.3301047,\n",
      "         'predicted number of factors': 0.919502,\n",
      "         'true factors': [(1.1, 'Job planning or instruction inadequate'),\n",
      "                          (5.0,\n",
      "                           '5. Incorrect body position in relation to work')],\n",
      "         'true number of actions': 2.0,\n",
      "         'true number of factors': 2.0}}\n"
     ]
    }
   ],
   "source": [
    "# Samples from the test dataset and prints the output for testing.\n",
    "cat_sample = X_cat_test.sample()\n",
    "num_sample = X_num_test.loc[cat_sample.index]\n",
    "\n",
    "pprint(get_predictions(cat_sample, num_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of all the available factor levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.0, 'Just and Fair Culture Matter'),\n",
       " (2.1, 'Rules, procedures or SWMS not followed'),\n",
       " (10.0, 'Environmental factors, weather'),\n",
       " (1.2, 'Inattention to details of job'),\n",
       " (13.0, 'Other Contributing Factors'),\n",
       " (1.1, 'Job planning or instruction inadequate'),\n",
       " (3.2, 'Guarding or protective devices not provided or ineffective'),\n",
       " (3.1, 'Design of plant, facilities, or equipment'),\n",
       " (3.0, 'Rules, procedures or JSA not followed'),\n",
       " (6.0, 'Housekeeping congested, incorrect storage'),\n",
       " (3.5, 'Improper vehicle operation'),\n",
       " (11.0, 'Inadequate knowledge or skill'),\n",
       " (5.0, '5. Incorrect body position in relation to work'),\n",
       " (12.0, 'Member of the public'),\n",
       " (3.3, 'Plant or equipment operated incorrectly'),\n",
       " (7.0, 'Incorrect or lack of Personal Protective Equipment'),\n",
       " (8.0, 'Inadequate knowledge or skill'),\n",
       " (2.2, 'Rules, procedures or SWMS inadequate'),\n",
       " (3.6, 'Maintenance, Inspection not adequate'),\n",
       " (2.0, 'Job planning or instruction inadequate'),\n",
       " (3.4, 'Incorrect tools or mechanical aids used'),\n",
       " (1.0, 'Design of plant, facilities, or equipment')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, factor_text(i)) for i in factor_mapping['factor-level']]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df54116b6d34ad4780a54da118c7de548406617ca1313d54eea7b41d49279628"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
