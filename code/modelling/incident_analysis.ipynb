{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incident Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Neural network model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory and Client Setup\n",
    "Configure with data directory and current client.\n",
    "\n",
    "The program generates models for each client individually for privacy reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r'../../data'\n",
    "\n",
    "# Change client in these statements\n",
    "COMPANY = 'geotec'\n",
    "from clients import Geotec as client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the set of text based and categorical predictors for later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRS = set(j for i in client for j in i.keys())\n",
    "TEXT_PREDICTORS = set(['description'])\n",
    "CATEGORICAL_PREDICTORS = set(i for i in client.incident_mapping \n",
    "                             if i not in TEXT_PREDICTORS) - set(['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Uses the mapping between excel columns and predictors (provided in `clients.py`) to extract the required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comma separated list of the required excel columns from each sheet.\n",
    "incident_excel_cols = ','.join(client.incident_mapping.values())\n",
    "action_excel_cols = ','.join(client.action_mapping.values())\n",
    "factor_excel_cols = ','.join(client.factor_mapping.values())\n",
    "\n",
    "# Get the corresponding column names in the order that they appear in the excel sheet.\n",
    "def sort_cols(mapping_dict):\n",
    "    return [k for k, v in sorted(mapping_dict.items(), key=lambda x: x[1])]\n",
    "\n",
    "incident_names = sort_cols(client.incident_mapping)\n",
    "action_names = sort_cols(client.action_mapping)\n",
    "factor_names = sort_cols(client.factor_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/lucas/Documents/Lucidity/incident_analysis/code/.venv/lib/python3.9/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/mnt/c/Users/lucas/Documents/Lucidity/incident_analysis/code/.venv/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "incidents = pd.read_excel(f'{DATA_DIR}/{COMPANY}-incidents.xlsx', \n",
    "                          usecols=incident_excel_cols, skiprows=9, names=incident_names)\n",
    "actions = pd.read_excel(f'{DATA_DIR}/{COMPANY}-actions.xlsx', \n",
    "                        usecols=action_excel_cols, skiprows=6, names=action_names)\n",
    "factors = pd.read_excel(f'{DATA_DIR}/{COMPANY}-factors.xlsx', \n",
    "                        usecols=factor_excel_cols, skiprows=5, names=factor_names)\n",
    "\n",
    "def process_ids(df):\n",
    "    \"\"\"Drops cols with no id's, sets integer id's, and mades id the index.\"\"\"\n",
    "    df = df.dropna(subset=['id'])\n",
    "    df.loc[:,'id'] = df.loc[:,'id'].astype('int64')\n",
    "    return df.set_index('id')\n",
    "\n",
    "incidents = process_ids(incidents)\n",
    "actions = process_ids(actions)\n",
    "full_factors = process_ids(factors)\n",
    "factors = full_factors.drop(['factor-text'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "Performs one-hot encoding on the categorical attributes to prepare the instances for input into ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_categoricals(df, categoricals: list[str], \n",
    "                         dummy_na=True, na_sentinel=None) -> tuple[pd.DataFrame, dict]:\n",
    "    # Remove the index ID (replaced on return)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Stores the mapping between new columns and existing categories\n",
    "    mappings = {}\n",
    "    for column in categoricals:\n",
    "        # Get integer categories\n",
    "        factorised, mapping = pd.factorize(df[column], na_sentinel=na_sentinel)\n",
    "        mappings[column] = mapping\n",
    "\n",
    "        # One-hot encode\n",
    "        dummies = pd.get_dummies(factorised, dummy_na=dummy_na, prefix=column)\n",
    "\n",
    "        # Replace the existing categorical col with the one-hots\n",
    "        df = df.drop([column], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "        dummies = dummies.reset_index(drop=True)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "    return df.set_index('id'), mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy of the incidents dataframe for ML\n",
    "df = incidents.copy()\n",
    "df, mapping = one_hot_categoricals(df, CATEGORICAL_PREDICTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of actions associated with each incident\n",
    "temp = incidents.join(actions, on='id')\n",
    "n_actions = temp.groupby('id').count()['action-id']\n",
    "\n",
    "# Repeat for factors\n",
    "temp = incidents.join(factors, on='id')\n",
    "n_factors = temp.groupby('id').count()['factor-level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the factor levels - possible memory issues with ridiculous number of factors\n",
    "temp, factor_mapping = one_hot_categoricals(temp, ['factor-level'], dummy_na=False, na_sentinel=-1)\n",
    "\n",
    "# factor_codes has one row for each instance, and 0-many columns with 1's representing\n",
    "# boolean flags of whether each factor (column) is present for that instance\n",
    "factor_cols = [col for col in temp.columns if 'factor-level_' in col]\n",
    "factor_codes = temp[factor_cols].groupby('id').sum()\n",
    "\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the target columns\n",
    "df = df.join(factor_codes, on='id')\n",
    "df['n_actions'] = n_actions\n",
    "df['n_factors'] = n_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding\n",
    "Uses a prebuilt text embedding model from Google to convert each text based predictor into a 50-dimensional vector, where nearby vectors in this 50-D space are assumed to be generated from strings with similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 09:38:48.713734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-16 09:38:48.809817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-16 09:38:48.810350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-16 09:38:48.815734: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-16 09:38:48.824796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-16 09:38:48.825592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-16 09:38:48.826641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-16 09:38:57.062315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-16 09:38:57.062797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-16 09:38:57.062813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2021-12-16 09:38:57.063202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-16 09:38:57.063253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5426 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "embedding_dim = 50\n",
    "embedding_model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "embedding_layer = hub.KerasLayer(embedding_model, input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(df, texts: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe and a list of n text columns. Returns a dataframe with\n",
    "    those n text columns replaced by 50n columns containing the aforementioned\n",
    "    text embedding.\n",
    "    \"\"\"\n",
    "    for column in texts:\n",
    "        res = np.zeros((len(df[column]), embedding_dim))\n",
    "\n",
    "        # Iterate over all the rows of the text column\n",
    "        for idx, txt in df.reset_index()[column].items():\n",
    "            if type(txt) is not str:\n",
    "                continue\n",
    "            \n",
    "            # If the cell is text, perform and store the embedding\n",
    "            res[idx] = embedding_layer([txt])[0]\n",
    "\n",
    "        # Add the embedding back into the dataframe\n",
    "        df = df.drop([column], axis=1)\n",
    "        col_names = [f'{column}_{i}' for i in range(50)]\n",
    "        df[col_names] = res\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the embedding and convert all the columns to floats now that all are numeric.\n",
    "df = embed_text(df, TEXT_PREDICTORS)\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save to excel for viewing\n",
    "# df.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "### Train / Val / Test Split\n",
    "Split the data for training, validation, and testing of the ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, temp = train_test_split(df, train_size=0.7, random_state=22)\n",
    "val, test = train_test_split(temp, train_size=0.5, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df):\n",
    "    \"\"\"\n",
    "    Takes a dataframe formatted from above, and splits it into the required\n",
    "    numeric and categorical predictors and targets for analysis.\n",
    "\n",
    "    Note that the categorical and numeric return values will not always have the\n",
    "    same amount of rows, since instances without factors are not included in the\n",
    "    categorical dataset (since the categories are factor levels and these\n",
    "    have no factors.)\n",
    "    \"\"\"\n",
    "    # Retain only columns with factors for the categorical dataset\n",
    "    cat_df = df[df['factor-level_-1'] == 0]\n",
    "    cat_df = cat_df.drop('factor-level_-1', axis=1)\n",
    "\n",
    "    # Split the predictors and targets\n",
    "    y_cat = cat_df[[col for col in cat_df.columns if 'factor-level_' in col]]\n",
    "    y_num = df[['n_actions', 'n_factors']]\n",
    "    X_cat = cat_df[[col for col in cat_df.columns if col not in y_num.columns and col not in y_cat.columns]]\n",
    "    X_num = df[[col for col in df.columns if col not in y_num.columns and col not in y_cat.columns]]\n",
    "\n",
    "    return X_cat, X_num, y_cat, y_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the split on each of the train, validation, and test datasets\n",
    "X_cat_train, X_num_train, y_cat_train, y_num_train = split_xy(train)\n",
    "X_cat_val, X_num_val, y_cat_val, y_num_val = split_xy(val)\n",
    "X_cat_test, X_num_test, y_cat_test, y_num_test = split_xy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Models\n",
    "#### Numeric Targets\n",
    "A model to predict the number of factors and actions for a provided instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses an input layer, two fully-connected hidden layers, and a 2D output layer\n",
    "num_model = Sequential([\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(80, activation='relu'),\n",
    "    Dense(2, activation='relu')  # ReLU since the n_{actions, factors} are +'ve\n",
    "])\n",
    "\n",
    "num_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 09:39:10.170270: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 3s 9ms/step - loss: 0.9514 - val_loss: 0.3438\n",
      "Epoch 2/200\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.6684 - val_loss: 0.3003\n",
      "Epoch 3/200\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.6207 - val_loss: 0.2889\n",
      "Epoch 4/200\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5855 - val_loss: 0.2798\n",
      "Epoch 5/200\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5559 - val_loss: 0.2911\n",
      "Epoch 6/200\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5314 - val_loss: 0.3106\n",
      "Epoch 7/200\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.4959 - val_loss: 0.2906\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "history = num_model.fit(X_num_train,\n",
    "                        y_num_train,\n",
    "                        epochs=200,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_num_val, y_num_val),\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a very similar model to predict the most likely factor levels\n",
    "cat_model = Sequential([\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(80, activation='relu'),\n",
    "    # Use a softmax activation on the output for categorical probabilities\n",
    "    Dense(len(y_cat_train.columns), activation='softmax')\n",
    "])\n",
    "\n",
    "cat_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0559 - val_loss: 0.0524\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0543 - val_loss: 0.0512\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0529 - val_loss: 0.0505\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0517 - val_loss: 0.0498\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0501 - val_loss: 0.0493\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0487 - val_loss: 0.0488\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 0.0487\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0466 - val_loss: 0.0485\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0454 - val_loss: 0.0488\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0447 - val_loss: 0.0489\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0435 - val_loss: 0.0487\n"
     ]
    }
   ],
   "source": [
    "# Train the categorical model stopping at minimum validation loss\n",
    "history = cat_model.fit(X_cat_train,\n",
    "                        y_cat_train,\n",
    "                        epochs=200,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_cat_val, y_cat_val),\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Get Predictions for an Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_text(factor_code):\n",
    "    \"\"\"Gets the last recorded factor description for a given factor code.\"\"\"\n",
    "    return full_factors[full_factors['factor-level'] == factor_code].iloc[-1]['factor-text']\n",
    "\n",
    "def reverse_factor_mapping(indices):\n",
    "    \"\"\"\n",
    "    Takes a set of integer factor codes (factor codes which have been \n",
    "    transformed from the original floating point codes into 0...n integers \n",
    "    through one-hot encoding) and returns the corresponding original factor\n",
    "    codes and factor text.\n",
    "    \"\"\"\n",
    "    reverse_map = factor_mapping['factor-level']\n",
    "    factor_codes = [reverse_map[idx] for idx in indices]\n",
    "\n",
    "    return [(code, factor_text(code)) for code in factor_codes]\n",
    "\n",
    "def get_predictions(cat_xs, num_xs, top=5, get_truth=True, incident_src=None):\n",
    "    \"\"\"\n",
    "    Takes two dataframes of categorical and numeric test instances and gets the \n",
    "    neural network predictions from both models.\n",
    "    \n",
    "    Returns a dictionary with:\n",
    "        Keys:\n",
    "            The keys from the input dataframe.\n",
    "        Values:\n",
    "            A dictionary containing:\n",
    "                - description: the provided text description\n",
    "                - predicted factors: a list of the top 5 (default) factors most\n",
    "                    likely to be included based on the provided input data (as\n",
    "                    predicted by the categorical neural network model.)\n",
    "                - predicted number of actions: as titled\n",
    "                - predicted number of factors: as titled\n",
    "                - [if get_truth] true factors: the true factor numbers and text\n",
    "                - [if get_truth] true number of actions: as titled\n",
    "                - [if get_truth] true number of factors: as titled\n",
    "    \"\"\"\n",
    "    cat_predictions = cat_model.predict(cat_xs)\n",
    "    num_predictions = num_model.predict(num_xs)\n",
    "\n",
    "    if incident_src is None:\n",
    "        incident_src = incidents\n",
    "\n",
    "    res = {}\n",
    "    for idx, pred in enumerate(cat_predictions):\n",
    "        item_id = cat_xs.index[idx]\n",
    "\n",
    "        # Get the indices of the factors with the largest probability\n",
    "        top_indices = np.argsort(pred)[:-top-1:-1]\n",
    "\n",
    "        # Get the probabilities of each index\n",
    "        top_probs = cat_predictions[idx][top_indices]\n",
    "        \n",
    "        # Get the relevant factor text\n",
    "        pred_text = reverse_factor_mapping(top_indices)\n",
    "\n",
    "        # Add the information to the dictionary\n",
    "        res[item_id] = {\n",
    "            'description': incident_src.loc[item_id]['description'], \n",
    "            'predicted factors': list(zip(pred_text, top_probs)), \n",
    "            'predicted number of actions': num_predictions[idx][0],\n",
    "            'predicted number of factors': num_predictions[idx][1]\n",
    "        }\n",
    "\n",
    "        if get_truth:\n",
    "            # Get information about the correct factor labels\n",
    "            true_row = df.loc[item_id][[col for col in df.columns if 'factor-level_' in col]]\n",
    "            true_factors = true_row[true_row == 1].index\n",
    "            true_codes = [int(factor.split('_')[-1]) for factor in true_factors]\n",
    "\n",
    "            true_text = reverse_factor_mapping(true_codes)\n",
    "\n",
    "            res[item_id]['true factors'] = true_text\n",
    "\n",
    "            res[item_id]['true number of actions'] = df.loc[item_id, 'n_actions']\n",
    "            res[item_id]['true number of factors'] = df.loc[item_id, 'n_factors']\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6322: {'description': 'Left Bank: Dewatering Pump – Diesel lost to ground '\n",
      "                       '(less than 5 litres). The pump was out of service had '\n",
      "                       'been placed in a staging area on the left bank in '\n",
      "                       'readiness for the Stage 2 River Diversion. The pump '\n",
      "                       'was positioned on unlevel ground causing the fuel in '\n",
      "                       'the fuel tank (within the skid of the pump) to move to '\n",
      "                       'the lower end near the fuel filling point. Fuel '\n",
      "                       'escaped from the small breather hole in the filler '\n",
      "                       'cap.',\n",
      "        'predicted factors': [((1.2, 'Inattention to details of job'),\n",
      "                               0.3214381),\n",
      "                              ((1.1, 'Job planning or instruction inadequate'),\n",
      "                               0.26404),\n",
      "                              ((2.1, 'Rules, procedures or SWMS not followed'),\n",
      "                               0.19535252),\n",
      "                              ((3.6, 'Maintenance, Inspection not adequate'),\n",
      "                               0.061174035),\n",
      "                              ((12.0, 'Member of the public'), 0.030546403)],\n",
      "        'predicted number of actions': 2.5930307,\n",
      "        'predicted number of factors': 2.0146701,\n",
      "        'true factors': [(1.2, 'Inattention to details of job')],\n",
      "        'true number of actions': 2.0,\n",
      "        'true number of factors': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# Samples from the test dataset and prints the output for testing.\n",
    "cat_sample = X_cat_test.sample()\n",
    "num_sample = X_num_test.loc[cat_sample.index]\n",
    "\n",
    "pprint(get_predictions(cat_sample, num_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of all the available factor levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.0, 'Just and Fair Culture Matter'),\n",
       " (2.1, 'Rules, procedures or SWMS not followed'),\n",
       " (10.0, 'Environmental factors, weather'),\n",
       " (1.2, 'Inattention to details of job'),\n",
       " (13.0, 'Other Contributing Factors'),\n",
       " (1.1, 'Job planning or instruction inadequate'),\n",
       " (3.2, 'Guarding or protective devices not provided or ineffective'),\n",
       " (3.1, 'Design of plant, facilities, or equipment'),\n",
       " (3.0, 'Rules, procedures or JSA not followed'),\n",
       " (6.0, 'Housekeeping congested, incorrect storage'),\n",
       " (3.5, 'Improper vehicle operation'),\n",
       " (11.0, 'Inadequate knowledge or skill'),\n",
       " (5.0, '5. Incorrect body position in relation to work'),\n",
       " (12.0, 'Member of the public'),\n",
       " (3.3, 'Plant or equipment operated incorrectly'),\n",
       " (7.0, 'Incorrect or lack of Personal Protective Equipment'),\n",
       " (8.0, 'Inadequate knowledge or skill'),\n",
       " (2.2, 'Rules, procedures or SWMS inadequate'),\n",
       " (3.6, 'Maintenance, Inspection not adequate'),\n",
       " (2.0, 'Job planning or instruction inadequate'),\n",
       " (3.4, 'Incorrect tools or mechanical aids used'),\n",
       " (1.0, 'Design of plant, facilities, or equipment')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, factor_text(i)) for i in factor_mapping['factor-level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create instances with fake descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_instance(description):\n",
    "    test_incident = pd.DataFrame({'description': [description]}, columns=incidents.columns)\n",
    "    test_incident.index.rename('id', inplace=True)\n",
    "\n",
    "    dummy_cols = [col for col in X_test.columns if not re.match(\"description_\", col)]\n",
    "    dummy_cols += ['description']\n",
    "    test_instance = pd.DataFrame({'description': [description]}, columns=dummy_cols)\n",
    "\n",
    "    test_instance = embed_text(test_instance, ['description']).fillna(0)\n",
    "\n",
    "    return test_incident, test_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and get predictions for fake instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_657/2620340298.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_incident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_test_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bob was not wearing safety goggles'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_657/1136087367.py\u001b[0m in \u001b[0;36mcreate_test_instance\u001b[0;34m(description)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_incident\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdummy_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"description_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdummy_cols\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "test_incident, test_instance = create_test_instance('bob was not wearing safety goggles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05491083 0.06921741 0.05070435 0.0982523  0.0951962  0.07404085\n",
      " 0.02881607 0.03856122 0.06665833 0.03609989 0.02325817 0.01883799\n",
      " 0.08037991 0.06353504 0.02461661 0.02037402 0.02442184 0.02081852\n",
      " 0.05791007 0.01842437 0.01744263 0.0175233 ]\n",
      "[ 3  4 12  5  1]\n",
      "{0: {'description': 'bob was not wearing safety goggles',\n",
      "     'predicted_factors': [(1.2, 'Inattention to details of job'),\n",
      "                           (13.0, 'Improper vehicle operation'),\n",
      "                           (5.0,\n",
      "                            '5. Incorrect body position in relation to work'),\n",
      "                           (1.1, 'Job planning or instruction inadequate'),\n",
      "                           (2.1, 'Rules, procedures or SWMS not followed')]}}\n"
     ]
    }
   ],
   "source": [
    "pprint(\n",
    "    get_factor_predictions(test_instance, \n",
    "                           incident_src=test_incident, \n",
    "                           get_true_factors=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df54116b6d34ad4780a54da118c7de548406617ca1313d54eea7b41d49279628"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
